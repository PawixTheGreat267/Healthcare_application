{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d98917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchinfo\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.version import __version__ as tqdm__version__\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fcdaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Platform:\", sys.platform)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"---\")\n",
    "print(\"CV2 version : \", cv2.__version__)\n",
    "print(\"matplotlib version : \", matplotlib.__version__)\n",
    "print(\"numpy version : \", np.__version__)\n",
    "print(\"torch version : \", torch.__version__)\n",
    "print(\"torchinfo version : \", torchinfo.__version__)\n",
    "print(\"torchvision version : \", torchvision.__version__)\n",
    "print(\"PIL version : \", PIL.__version__)\n",
    "print(\"scikit-learn version: \", sklearn.__version__)\n",
    "print(\"tqdm version: \", tqdm__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d27421",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"data_p1\", \"data_binary\")\n",
    "\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "\n",
    "labels = os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_path = os.path.join(train_dir, \"hog\")\n",
    "hog_images = os.listdir(hog_path)\n",
    "\n",
    "print(\"length of hog images: \", len(hog_images))\n",
    "\n",
    "blank_path = os.path.join*train_dir, \"blank\")\n",
    "blank_images = os.listdir(blank_path)\n",
    "\n",
    "print(\"length of blank images: \", len(blank_images))\n",
    "\n",
    "\n",
    "hog_image_name = hog_images[0]\n",
    "print(hog_image_name)\n",
    "print(\"hog image name: \", hog_image_name)\n",
    "\n",
    "hog_image_path = os.path.join(hog_path, hog_image_name)\n",
    "print(hog_image_path)\n",
    "\n",
    "\n",
    "blank_image_name = blank_images[0]\n",
    "print(blank_image_name)\n",
    "print(\"blank image name: \" , blank_image_name)\n",
    "blank_image_path = os.path.join(blank_path, blank_image_name)\n",
    "print(blank_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_img_pil = Image.open(hog_image_path)\n",
    "print(\"Hog image: \", hog_img_pil)\n",
    "\n",
    "\n",
    "blank_img_pil = Image.open(blank_image_path)\n",
    "print(\"Blank image: \", blank_img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea39b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToRGB:\n",
    "    def __call__(self, img):\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    ConvertToRGB(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "\n",
    "print(type(transforms))\n",
    "print(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f8e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=train_dir, transform=transforms)\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a50965",
   "metadata": {},
   "source": [
    "Prove that only distinct values of im are 0 and 1. You should use set data structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = dataset.imgs\n",
    "print(im[0])\n",
    "\n",
    "\n",
    "#To get that the labels are only 0 and 1 \n",
    "distinct_classes = set{x[1] for x in im}\n",
    "print(\"Distinct classes: \", distinct_classes)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d54de",
   "metadata": {},
   "source": [
    "Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce815198",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=g)\n",
    "print(f \"length of train dataset: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33290e",
   "metadata": {},
   "source": [
    "It's good to explore the data. We'll create a visualization to show the breakdown of the two classes. The function below goes through the dataset and counts how many images are in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts():\n",
    "    c = Counter(x[1] for x in tqdm(dataset))\n",
    "    class_to_index = dataset.dataset.class_to_idx\n",
    "    return pd.Series({cat: c[idx] for cat, idx in class_to_index.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = class_counts(train_dataset)\n",
    "train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts.sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = class_counts(val_dataset)\n",
    "val_counts\n",
    "val_counts.sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff98968",
   "metadata": {},
   "source": [
    "Creating a Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "batch_size = 32 \n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, generator=g)\n",
    "print(f\"length of train loader: {len(train_loader)}\")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, generator=g)\n",
    "print(f\"length of validation loader: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86383504",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "image_shape = images.shape\n",
    "print(\"Images shape: \", image_shape)\n",
    "\n",
    "label_shape = labels.shape\n",
    "print(\"Labels shape: \", label_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fa93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739c614",
   "metadata": {},
   "source": [
    "<b>Building a Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c79103",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of flattening the image since the images are 3D tensors, we need to flatten them to 1D for the neural network\n",
    "flatten = nn.Flatten()\n",
    "\n",
    "tensor_flatten = flatten(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ef867",
   "metadata": {},
   "source": [
    "Using sequential network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1bf541",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 224\n",
    "weight = 224 \n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(3 * height * width, 512), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "print(\"model type: \", type(model))\n",
    "print(\"model structure:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing the last layer which is the output layer \n",
    "output_layer = nn.Linear(128, 2)\n",
    "\n",
    "model.append(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb0478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the model into the device for cuda or gpu runtime\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a50a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(batch_size, 3,height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d7285",
   "metadata": {},
   "source": [
    "<b>The loss function measures how well our model does for a given set of model parameters</b>. The chosen loss function, cross-entropy, is pretty standard. It's the same loss function used for simpler machine learning models such as logistic regression. Note that this function expects the input to be logits from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d0d85",
   "metadata": {},
   "source": [
    "We also need an <b>optimizer</b>. This will adjust the model's parameters to try to minimize the loss function. We've chosen the Adam optimizer, a popular optimizer. The Adam optimizer is a gradient based optimizer like stochastic gradient descent. The Adam optimizer has additional features that make it less likely to get stuck in a local minimum. It converges to a better state faster than standard stochastic gradient descent. The optim.Adam class is initialized with the model parameters through model.parameters. An optional argument is the learning rate lr. This controls how large the step sizes are in gradient descent. Keeping the default value will be fine for our purposes. We've explicitly specified the default value in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7671701",
   "metadata": {},
   "source": [
    "Now we'll create a function called train_epoch that encapsulates the training process. The code is daunting at first but we're going to break it down. The function accepts\n",
    "\n",
    "<br>model: The PyTorch model we built with a specific architecture\n",
    "<br>optimizer: The optimizer that will be used to best adjust the model weights\n",
    "<br>loss_fn: The loss function that the optimizer is trying to minimize\n",
    "<br>data_loader: The DataLoader object for the training dataset that makes it easy to iterate over batches\n",
    "<br>device: The device where we're going to place the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32950576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, loss_fn, data_loader, device='cpu'):\n",
    "    # We'll report the loss function's average value at the end of the epoch.\n",
    "    training_loss = 0.0\n",
    "\n",
    "    #The train method simply sets the model in training mode. No training has happened.\n",
    "    model.train()\n",
    "\n",
    "    # We iterate over all batches in the training set to complete one epoch\n",
    "    for inputs, targets in tqdm(data_loader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Unpack images (X) and labels (y) from the batch and add those\n",
    "    # tensors to the specified device.\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # We make a forward pass through the network and obtain the logits.\n",
    "    # With the logits, we can calculate our loss.\n",
    "    output = model(inputs)\n",
    "    loss = loss_fn(output, targets)\n",
    "\n",
    "    # After calculating our loss, we calculate the numerical value of\n",
    "    # the derivative of our loss function with respect to all the\n",
    "    # trainable model weights. Once we have the gradients calculated,\n",
    "    # we let the optimizer take a \"step\", in other words, update or\n",
    "    # adjust the model weights.\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # We increment the training loss for the current batch\n",
    "    training_loss += loss.data.item() * input.size(0)\n",
    "    \n",
    "    #we calculate the training loss over the completed batch\n",
    "    return training_loss / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd5c6e",
   "metadata": {},
   "source": [
    "The tqdm function that is wrapped around the data_loader will give us a progress bar that fills up as we process the data. It's not necessary for the training process. It's just there to reassure us that something is actually happening!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbcabb",
   "metadata": {},
   "source": [
    "We'll train the model for one epoch. The train_epoch function returns the average training loss, cross-entropy, for the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value = train_epoch(model, optimizer, loss_fn, train_loader, device)\n",
    "print(f\"The average loss during the training epoch was {loss_value:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a90444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, device='cpu'):\n",
    "    #The tensor will store all the predictions\n",
    "    all_probs = torch.tensor([]).to(device)\n",
    "\n",
    "    #We set the model to evaluation mode. This mode is the opposite of the train mode we set in the train_epoch function\n",
    "    model.eval()\n",
    "\n",
    "    # Since we're not training, we don't need any gradient calculations.\n",
    "    # This tells PyTorch not to calculate any gradients, which speeds up\n",
    "    # some calculations.\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for inputs, targets in tqdm(data_loader, desc='Predicting', leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "\n",
    "            # The model produces the logits.  This softmax function turns the\n",
    "            # logits into probabilities.  These probabilities are concatenated\n",
    "            # into the `all_probs` tensor.\n",
    "\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            all_probs = torch.cat((all_probs, probs), dim=0)\n",
    "    \n",
    "    return all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_train = predict(model, train_loader, device)\n",
    "print(probabilities_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d9ac83",
   "metadata": {},
   "source": [
    "torch.size([2553, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006286ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_val = predict(model, val_loader, device)\n",
    "print(probabilities_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_probability = probabilities_val[0].sum()\n",
    "print(f\"Sum of probabilities: {total_probability.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd4286a",
   "metadata": {},
   "source": [
    "torch.size([638,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2d5ee",
   "metadata": {},
   "source": [
    "To make a prediction from these probabilities, we predict the class with the highest probability for each row. This can be done with the torch.argmax function, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942680b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = torch.argmax(probabilities_train, dim=1)\n",
    "\n",
    "print(f\"Predictions shape: {predictions_train.shape}\")\n",
    "print(f\"First 10 predictions: {predictions_train[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97210d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_val = torch.argmax(probabilities_val, dim=1)\n",
    "\n",
    "print(f\"Predictions shape: {predictions_val.shape}\")\n",
    "print(f\"First 10 predictions: {predictions_val[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0358e",
   "metadata": {},
   "source": [
    "Get the accurate prediction on the probabilities of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_train = torch.cat([labels for _, labels in train_loader]).to(device)\n",
    "is_correct_train = torch.eq(predictions_train, targets_train)\n",
    "total_correct_train = torch.sum(is_correct_train).item()\n",
    "accuracy_train =  total_correct_train / len(train_loader.dataset)\n",
    "\n",
    "print(f\"Accuracy on the training data: {accuracy_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d83b7",
   "metadata": {},
   "source": [
    "Accuracy on the training data: 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_val = torch.cat([labels for _, labels in val_loader]).to(device)\n",
    "is_correct_val = torch.eq(predictions_val, targets_val)\n",
    "total_correct_val = torch.sum(is_correct_val)\n",
    "accuracy_val = total_correct_val / len(val_loader.dataset)\n",
    "\n",
    "print(f\"Accuracy on the valdiation data: {accuracy_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ba553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, data_loader, loss_fn, device=\"cpu\"):\n",
    "    # Initialize the total loss (cross entropy) and the number of correct\n",
    "    # predictions. We'll increment these values as we loop through the\n",
    "    # data.\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    # We set the model to evaluation mode. This mode is the opposite of\n",
    "    # train mode we set in the train_epoch function.\n",
    "    model.eval()\n",
    "\n",
    "    # Since we're not training, we don't need any gradient calculations.\n",
    "    # This tells PyTorch not to calculate any gradients, which speeds up\n",
    "    # some calculations.\n",
    "    with torch.no_grad():\n",
    "        # We iterate over the batches in the data loader and feed\n",
    "        # them into the model for the forward pass.\n",
    "        for inputs, targets in tqdm(data_loader, desc=\"Scoring\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "\n",
    "            # Calculating the loss function for this batch\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output, targets)\n",
    "            total_loss += loss.data.item() * inputs.size(0)\n",
    "\n",
    "            # Calculating the correct predictions for this batch\n",
    "            correct = torch.eq(torch.argmax(output, dim=1), targets)\n",
    "            total_correct += torch.sum(correct).item()\n",
    "\n",
    "    return total_loss / len(data_loader.dataset), total_correct / len(\n",
    "        data_loader.dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcdef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train, accuracy_train = score(model, train_loader, loss_fn, device)\n",
    "print(f\"Training accuracy from score function: {accuracy_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372407a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val accuracy_val = score(model, train_loader, loss_fn, device)\n",
    "print(f\"Training accuracy from score function: {accuracy_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3951a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Run train_epoch once, and capture the training loss.\n",
    "        training_loss = train_epoch(model, optimizer, loss_fn, train_loader, device)\n",
    "\n",
    "        # Score the model on the validation data.\n",
    "        validation_loss, validation_accuracy = score(model, val_loader, loss_fn, device)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Training Loss: {training_loss:.2f}, \"\n",
    "            f\"Validation Loss: {validation_loss:.2f}, Validation Accuracy: {validation_accuracy:.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, loss_fn, train_loader, val_loader, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"model/trained_model.pth\", weights_only=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
