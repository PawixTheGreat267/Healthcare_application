{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd406bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import PIL \n",
    "import torch \n",
    "import torchvision\n",
    "from  PIL import Image\n",
    "from torchvision import transformers \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e16f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"data_p1\", \"data_multiclass\")\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "\n",
    "\n",
    "class_directories = os.listdir(train_dir)\n",
    "print(\"class_directories type:\", type(class_directories))\n",
    "print(\"class_directories length:\", len(class_directories))\n",
    "print(class_directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a359702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution_list = {}\n",
    "\n",
    "for subdirectory in class_directories:\n",
    "    dir = os.path.join(train_dir, subdirectory)\n",
    "    files = os.listdir(dir)\n",
    "    num_files = len(files)\n",
    "    class_distribution_list[subdirectory]  = num_files\n",
    "\n",
    "class_distribution_list = pd.Series(class_distribution_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22cdfe",
   "metadata": {},
   "source": [
    "<b>PRACTICE RECAP NEURAL NETWORK BINARY CLASSIFICATION</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48fc7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Platform:\", sys.platform)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"---\")\n",
    "print(\"matplotlib version:\", matplotlib.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"PIL version:\", PIL.__version__)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torchvision version:\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93519e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa9b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToRGB:\n",
    "    def __call__(self, img):\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c7ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        ConvertToRGB(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7887fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data_p1/data_multiclass\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "\n",
    "print(\"Will read data from:\", train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=train_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e582f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classes:\")\n",
    "print(dataset.classes)\n",
    "print(f\"That's{len(dataset.classes)} classes.\")\n",
    "\n",
    "print()\n",
    "print(\"Tensor shape for one image:\")\n",
    "print(dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f19c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "\n",
    "dataset_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "first_batch = next(iter(dataset_loader))\n",
    "\n",
    "\n",
    "print(f\"Shape of one batch: {first_batch[0].shape}\")+\n",
    "print(f\"Shape of labels: {first_batch[1].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_loader(loader):\n",
    "\n",
    "    channel_sum, channels_squared_sum, num_batches = 0,0,0\n",
    "    for data, _ in tqdm(loader, desc=\"Calculating mean and std\", leave=False):\n",
    "        channel_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "\n",
    "        num_batches += 1\n",
    "    mean = channel_sum / num_batches\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "    return mean, std   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = get_mean_std_loader(dataset_loader)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_norm = transforms.Compose(\n",
    "    ConvertToRGB(),\n",
    "    transforms.Resize((224,224))\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dataset=datasets.ImageFolder(root=train_dir, transform=transform_norm)\n",
    "\n",
    "norm_loader= DataLoader(norm_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32922e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mean, norm_std = get_mean_std_loader(norm_loader)\n",
    "\n",
    "print(f\"Norm Mean: {norm_mean}\")\n",
    "print(f\"Norm Std: {norm_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c2735",
   "metadata": {},
   "source": [
    "Data Splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b195b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "train_dataset, val_dataset = random_split(norm_dataset, [0.8, 0.2], generator=g)\n",
    "\n",
    "length_train = len(train_dataset)\n",
    "length_val = len(val_dataset)\n",
    "length_dataset = len(norm_dataset)\n",
    "percent_train = np.round(length_train / length_dataset * 100, 2)\n",
    "percent_val = np.round(length_val / length_dataset * 100, 2)\n",
    "\n",
    "print(f\"Train data is {percent_train}% of the full data\")\n",
    "print(f\"Validation data is {percent_val}% of the full data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56778764",
   "metadata": {},
   "source": [
    "Visualization of each category in the data splitting \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts(dataset):\n",
    "    c= Counter(x[1] for x in tqdm(dataset))\n",
    "    class_to_index = dataset.dataset.class_to_idx\n",
    "    return pd.Series({cat: c[idx] for cat, idx in class_to_index.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167eb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_distributions = class_counts(train_dataset)\n",
    "\n",
    "train_class_distributions\n",
    "\n",
    "\n",
    "##Using barplot  for training class distribution visualization\n",
    "train_class_distributions.sort_values().plot(kind='bar')\n",
    "\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Frequency[count]\")\n",
    "plt.title(\"Class Distribution in Training Set\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##Using barplot  for validation class distribution visualization\n",
    "val_class_distributions = class_counts(val_dataset)\n",
    "\n",
    "val_class_distributions\n",
    "\n",
    "val_class_distributions.sort_values().plot(kind='bar')\n",
    "\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Frequency[count]\")      \n",
    "plt.title(\"Class Distribution in Validation Set\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5342dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "single_batch = next(iter(train_loader))[0]\n",
    "print(f\"Shape of one batch: {single_batch.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be20b9",
   "metadata": {},
   "source": [
    "Multiclass Predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_confidence = torch.tensor([0.13, 0.01, 0.02, 0.12, 0.10, 0.34, 0.16, 0.12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46dd8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = norm_dataset.classes\n",
    "\n",
    "class_number = torch.argmax(sample_confidence)\n",
    "prediction = classes[class_number]\n",
    "\n",
    "print(f \"This image is a {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673c277c",
   "metadata": {},
   "source": [
    "Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq = nn.Sequential()\n",
    "\n",
    "conv1= torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_Size=(3,3), padding=1)\n",
    "model_seq.append(conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bade9c",
   "metadata": {},
   "source": [
    "Let's also get a single batch of 32 images, so we can see what the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db47d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(train_loader))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdeb98",
   "metadata": {},
   "source": [
    "Let's make sure the batch is the shape we expect. It should be 32 images, with 3 color channels, of size 224 x 224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f98583",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shape = test_batch.shape\n",
    "\n",
    "print(f\"Batch shape: {batch_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ace27f",
   "metadata": {},
   "source": [
    "Get the shape of the output and store it to first_step_shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6113cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_step_out = model_seq(test_batch)\n",
    "\n",
    "print(f\"Shape after first conv layer: {first_step_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071ea96",
   "metadata": {},
   "source": [
    "Shape after first convolution layer: torch.Size([32, 16, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad241d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need an activation function after the convolutional layer to introduce non-linearity.\n",
    "model_seq.append(nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b2135",
   "metadata": {},
   "source": [
    "Our convolution layers transform images into more images. Ultimately, we're going to need to get down to just our 8 output classes. But our convolution has increased the number of pixels! Max pooling will let us shrink our image.\n",
    "\n",
    "In PyTorch, this is a MaxPool2D layer. The 2D is because we're leaving the channels alone, so it'll max pool on each of our \n",
    "16 channels separately. We'll need to say how big of a patch to reduce, called the kernel again. We'll set it to 2x2, a standard choice. We'll set our stride to 2 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ce4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool1 = torch.nn.MaxPool2d(kernel_size=(2,2),stride=2)\n",
    "\n",
    "model_seq.append(max_pool1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a56cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.0' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/ACER/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Run again on a test batch\n",
    "\n",
    "max_pool_out = model_seq(test_batch)\n",
    "max_pool_shape =max_pool_out.shape\n",
    "print(f\"Shape after max pooling: {max_pool_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15970491",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = torch.nn.Conv2D(in_channels=16, out_channels=32, kernel_size=(3,3), padding=1)\n",
    "max_pool2 = torch.nn.MaxPool2D(kernel_size=(2,2), stride=2)\n",
    "\n",
    "model_seq.append(conv2)\n",
    "model_seq.append(torch.nn.ReLU())\n",
    "model_seq.append(max_pool2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831fadfd",
   "metadata": {},
   "source": [
    "Run the current model on the test_batch, and save the output's shape to second_set_shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_set_out = model_seq(test_batch)\n",
    "second_set_shape = second_set_out.shape\n",
    "\n",
    "print(f\"Shape after second conv and max pooling: {second_set_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d0cae",
   "metadata": {},
   "source": [
    "Shape after second max pool: torch.Size([32, 32, 111, 111])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd502e1",
   "metadata": {},
   "source": [
    "You can actually simplify these descriptions. First, you don't have to provide the argument names for most things, IF you provide them in the right order (padding is an exception). Second, for the kernels, if you say 2 it knows you mean 2 x 2 (same for 3, etc). Finally, for the max pool you can leave off the stride, it defaults to the size of the kernel. We can use this to make the description of our third layer set more compact. We'll use 64 kernels this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ae700",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3 = torch.nn.Conv2d(32, 64, 3, padding=1)\n",
    "max_pool3 = torch.nn.MaxPool2d(2)\n",
    "\n",
    "model_seq.append(conv3)\n",
    "model_seq.append(torch.nn.ReLU())\n",
    "model_seq.append(max_pool3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586a467",
   "metadata": {},
   "source": [
    "Sequential(<br>\n",
    "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) <br>\n",
    "  (1): ReLU()<br>\n",
    "  (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)<br>\n",
    "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>\n",
    "  (4): ReLU()<br>\n",
    "  (5): MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)<br>\n",
    "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>\n",
    "  (7): ReLU()<br>\n",
    "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br>\n",
    ")<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257eb97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_set_out = model_seq(test_batch)\n",
    "third_set_shape = third_set_out.shape\n",
    "\n",
    "print(f\"Shape after third conv and max pooling: {third_set_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a13c1",
   "metadata": {},
   "source": [
    "Shape after third max pool: torch.Size([32, 64, 55, 55])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc482d",
   "metadata": {},
   "source": [
    "<b>Finishing the model</b>\n",
    "We're going to need an output layer with just 8 neurons. That's a flat output, without the 3D structure of our images. Conveniently, PyTorch provides a Flatten layer for flattening. Let's add that to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq.append(torch.nn.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03b5ac",
   "metadata": {},
   "source": [
    "Sequential( <br>\n",
    "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) <br>\n",
    "  (1): ReLU() <br>\n",
    "  (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False) <br>\n",
    "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) <br>\n",
    "  (4): ReLU() <br>\n",
    "  (5): MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False) <br>\n",
    "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) <br>\n",
    "  (7): ReLU() <br>\n",
    "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) <br>\n",
    "  (9): Flatten(start_dim=1, end_dim=-1) <br>\n",
    ") <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e7aca",
   "metadata": {},
   "source": [
    "flat_out = model_seq(test_batch)\n",
    "flat_shape = flat_out.shape\n",
    "\n",
    "print(f\"Shape after flattening: {flat_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a2818",
   "metadata": {},
   "source": [
    "Shape after flattening: torch.Size([32, 50176])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88f747",
   "metadata": {},
   "source": [
    "At this point we have a flat input, and can build a normal set of dense layers. You can think of the convolution/max pool layers as having done the image processing. Now we need to do the actual classification. It turns out that dense layers are good at that task.\n",
    "\n",
    "We could add a single layer and just go straight to our output 8 classes. But we'll get better performance by adding a few dense layers, Linear in PyTorch's terminology, first. For these layers, we need to tell it the size of the input, and how many neurons we want in the layer. Since the input is our previous layer, we tell it that size. We'll add a layer of 500 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(in_features=50176, out_features=500)\n",
    "\n",
    "model_seq.append(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9a18d",
   "metadata": {},
   "source": [
    "Sequential(<br>\n",
    "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>\n",
    "  (1): ReLU()<br>\n",
    "  (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)<br>\n",
    "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>\n",
    "  (4): ReLU()<br>\n",
    "  (5): MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)<br>\n",
    "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>\n",
    "  (7): ReLU()<br>\n",
    "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br>\n",
    "  (9): Flatten(start_dim=1, end_dim=-1)<br>\n",
    "  (10): Linear(in_features=50176, out_features=500, bias=True)<br>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923bc94d",
   "metadata": {},
   "source": [
    "model_seq.append(torch.nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696829a",
   "metadata": {},
   "source": [
    "Sequential(<br>\n",
    "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>\n",
    "  (1): ReLU()<br>\n",
    "  (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)<br>\n",
    "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>\n",
    "  (4): ReLU()<br>\n",
    "  (5): MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)<br>\n",
    "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>\n",
    "  (7): ReLU()<br>\n",
    "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br>\n",
    "  (9): Flatten(start_dim=1, end_dim=-1)<br>\n",
    "  (10): Linear(in_features=50176, out_features=500, bias=True)<br>\n",
    "  (11): ReLU()<br>\n",
    ")<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_out = model_seq(test_batch)\n",
    "linear_shape = linear_out.shape\n",
    "\n",
    "print(f\"Shape after linear layer: {linear_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612a87d",
   "metadata": {},
   "source": [
    "Shape after linear layer: torch.Size([32, 500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91fa78",
   "metadata": {},
   "source": [
    " Add the output dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c7dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = torch.nn.Linear(in_features=500, out_features=8, bias=True)\n",
    "\n",
    "model_seq.append(output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83382faf",
   "metadata": {},
   "source": [
    "Sequential( <br>\n",
    "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>\n",
    "  (1): ReLU()<br>\n",
    "  (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)<br>\n",
    "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>\n",
    "  (4): ReLU()<br>\n",
    "  (5): MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)<br>\n",
    "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>\n",
    "  (7): ReLU()<br>\n",
    "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br>\n",
    "  (9): Flatten(start_dim=1, end_dim=-1)<br>\n",
    "  (10): Linear(in_features=193600, out_features=500, bias=True)<br>\n",
    "  (11): ReLU()<br>\n",
    "  (12): Linear(in_features=500, out_features=8, bias=True)<br>\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc406f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq(test_batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([32, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb54c2",
   "metadata": {},
   "source": [
    "<b>Training the Model</b>\n",
    "Before we start training, let's put all the model code in one place. This is how you'd do it in practice, to prevent errors.\n",
    "\n",
    "We have also added Dropout layers after the flattened and linear layers. This helps to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important! Don't change this\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model = torch.nn.Sequential()\n",
    "\n",
    "conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "max_pool1 = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "model.append(conv1)\n",
    "model.append(torch.nn.ReLU())\n",
    "model.append(max_pool1)\n",
    "\n",
    "conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "max_pool2 = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "model.append(conv2)\n",
    "model.append(torch.nn.ReLU())\n",
    "model.append(max_pool2)\n",
    "\n",
    "conv3 = torch.nn.Conv2d(32, 64, 3, padding=1)\n",
    "max_pool3 = torch.nn.MaxPool2d(2)\n",
    "model.append(conv3)\n",
    "model.append(torch.nn.ReLU())\n",
    "model.append(max_pool3)\n",
    "\n",
    "model.append(torch.nn.Flatten())\n",
    "model.append(torch.nn.Dropout())\n",
    "\n",
    "linear1 = torch.nn.Linear(in_features=50176, out_features=500)\n",
    "model.append(linear1)\n",
    "model.append(torch.nn.ReLU())\n",
    "model.append(torch.nn.Dropout())\n",
    "\n",
    "output_layer = torch.nn.Linear(500, 8)\n",
    "model.append(output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359056d",
   "metadata": {},
   "source": [
    "Sequential( <Br>\n",
    "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<Br>\n",
    "  (1): ReLU()<Br>\n",
    "  (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)<Br>\n",
    "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<Br>\n",
    "  (4): ReLU()<Br>\n",
    "  (5): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)<Br>\n",
    "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<Br>\n",
    "  (7): ReLU()<Br>\n",
    "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<Br>\n",
    "  (9): Flatten(start_dim=1, end_dim=-1)<Br>\n",
    "  (10): Dropout(p=0.5, inplace=False)<Br>\n",
    "  (11): Linear(in_features=50176, out_features=500, bias=True)<Br>\n",
    "  (12): ReLU()<Br>\n",
    "  (13): Dropout(p=0.5, inplace=False)<Br>\n",
    "  (14): Linear(in_features=500, out_features=8, bias=True)<Br>\n",
    ")<Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555d0fe",
   "metadata": {},
   "source": [
    "And to make sure PyTorch has the model correct, let's look at the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aaf516",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = 224, 224\n",
    "summary(model, input_size=(batch_size, 3, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b085f8",
   "metadata": {},
   "source": [
    "This model has over 25 million parameters. That's fewer than our previous model, but because of the more complicated architecture it'll take more time and resources to train.\n",
    "\n",
    "We can use the same training code we used last time. It can handle binary or multiclass classification. We made a separate file with this code in the previous notebook. Now we can reuse that code by importing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b42065",
   "metadata": {},
   "source": [
    "from training import predict, train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09ea7bf",
   "metadata": {},
   "source": [
    "As in the previous notebook, we'll use the cross entropy as our loss function. This will take into account how confident the model is in its answer, as well as whether it was right or wrong. We will also print the accuracy as a human-readable measure.\n",
    "\n",
    "We'll need to set up our Cross Entropy loss, and an optimizer. We'll also make sure our model is on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df81061",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03149a27",
   "metadata": {},
   "source": [
    "Sequential( <Br>\n",
    "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<Br>\n",
    "  (1): ReLU()<Br>\n",
    "  (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)<Br>\n",
    "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<Br>\n",
    "  (4): ReLU()<Br>\n",
    "  (5): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)<Br>\n",
    "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<Br>\n",
    "  (7): ReLU()<Br>\n",
    "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<Br>\n",
    "  (9): Flatten(start_dim=1, end_dim=-1)<Br>\n",
    "  (10): Dropout(p=0.5, inplace=False)<Br>\n",
    "  (11): Linear(in_features=50176, out_features=500, bias=True)<Br>\n",
    "  (12): ReLU()<Br>\n",
    "  (13): Dropout(p=0.5, inplace=False)<Br>\n",
    "  (14): Linear(in_features=500, out_features=8, bias=True)<Br>\n",
    ")<Br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, loss_fn, train_loader, val_loader, epochs=8, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a23512",
   "metadata": {},
   "source": [
    "Pretrained Model: Load the pre-trained model with the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"model/trained_model.pth\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cde236",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = predict(model, val_loader, device)\n",
    "predictions = torch.argmax(probabilities, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f65b6",
   "metadata": {},
   "source": [
    "With those and the correct answers, we can generate the confusion matrix. Let's pull the targets into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "\n",
    "for _, labels in tqdm(val_loader):\n",
    "    targets.extend(labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18125231",
   "metadata": {},
   "source": [
    " Make the same confusion matrix we made last time. You'll need to either move the predictions to cpu or convert them to a list. The labels will be our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(targets, predictions.cpu())\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation=\"vertical\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4958e94",
   "metadata": {},
   "source": [
    "For testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f94c7",
   "metadata": {},
   "source": [
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "# Read the CSV file\n",
    "id_file_location = os.path.join(test_dir, \"test_features.csv\")\n",
    "df_ids = pd.read_csv(id_file_location)\n",
    "\n",
    "df_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf82f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_location = df_ids.iloc[0, 1]\n",
    "test_image_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709bc926",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = os.path.join(test_dir, test_image_location)\n",
    "test_image = PIL.Image.open(test_image_path)\n",
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d60349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_trans = transform_norm(test_image)\n",
    "test_image_trans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d9cd7",
   "metadata": {},
   "source": [
    "torch.Size([3, 224, 224])\n",
    "\n",
    "\n",
    "\n",
    "Almost there. Our model is actually expecting a batch of these. Since we only have one, we'll need to change the tensor to a 1 x 3 x 224 x 224\n",
    ". We could do this with reshape, but it's easier to use unsqueeze. unsqueeze is meant for this exact problem. It adds an extra dimension with one element. We just specify which extra dimension we want. In our case, we want the first dimension (i.e. 0) to be the extra one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unsqueeze = test_image_trans.unsqueeze(0)\n",
    "test_unsqueeze.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069d274",
   "metadata": {},
   "source": [
    "torch.Size([1, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e25e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_cuda = test_unsqueeze.to(device)\n",
    "test_out = model(test_image_cuda)\n",
    "test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4fb771",
   "metadata": {},
   "source": [
    "tensor([[ 2.7270, -6.9468,  2.9463, -0.1695, -7.5702, -3.2693, -2.5679,  1.5404]],\n",
    "       device='cuda:0', grad_fn=<AddmmBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71363b6c",
   "metadata": {},
   "source": [
    "That's the model's predictions, but it's not in the form we expected. We want a set of numbers between 0 and 1, that sum to 1. What we're seeing is the raw output of the last layer. To convert this to the confidences, we need to run it through a SoftMax. This is very much like the logistic or sigmoid you've seen before, except that it works with many inputs. The dim=1 tells it each row is one prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_softmax = torch.nn.functional.softmax(test_out, dim=1)\n",
    "test_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b3d20",
   "metadata": {},
   "source": [
    "Now that we have our numbers, we need to convert to a DataFrame. The predictions are made in the same order as the classes in our dataset. We can't convert a tensor directly to a DataFrame, so we'll convert it to a list first. Then we can set the columns to our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fde183",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_softmax.tolist())\n",
    "test_df.columns = dataset.classes\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d776291",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = df_ids.iloc[0, 0]\n",
    "test_df.index = [image_id]\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf51ce6",
   "metadata": {},
   "source": [
    "Let's put this into a function to make it easier. We'll also add a few things to make PyTorch run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "\n",
    "def file_to_confidence(file_path, image_id):\n",
    "    image = PIL.Image.open(file_path)\n",
    "    transformed = transform_norm(image)\n",
    "    unsqueezed = transformed.unsqueeze(0)\n",
    "    image_cuda = unsqueezed.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model_raw = model(image_cuda)\n",
    "        confidence = torch.nn.functional.softmax(model_raw, dim=1)\n",
    "\n",
    "    conf_df = pd.DataFrame(confidence.tolist())\n",
    "    conf_df.columns = dataset.classes\n",
    "    conf_df.index = [image_id]\n",
    "\n",
    "    return conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5415721",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_confidence(test_image_path, image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8c389",
   "metadata": {},
   "source": [
    " Do it! You can use df_ids.itertuples() to get one row at a time, and pd.concat to assemble many DataFrames into one big one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3df433",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dfs = []\n",
    "\n",
    "for row in df_ids.itertuples():\n",
    "    image_id = row.id\n",
    "    file_loc = row.filepath\n",
    "    filepath = os.path.join(test_dir, file_loc)\n",
    "    small_dfs.append(file_to_confidence(filepath, image_id))\n",
    "\n",
    "confidence_df = pd.concat(small_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb36bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c9d45",
   "metadata": {},
   "source": [
    "<b>Saving the Model</b>\n",
    "We've put a lot of effort into this model, and it took a while to train. The training has determined what the best parameters (also called weights) for our network. The only information we need to reproduce it's the network architecture, and the values of those parameters. PyTorch lets us save all of this. Then we can just load the model in the future instead of having to retrain it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484be2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model/deepnet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
